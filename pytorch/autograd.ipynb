{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Tutorial\n",
    "MILA, November 2017\n",
    "\n",
    "### Torch Autograd, Variables, Define-by-run & Execution Paradigm\n",
    "\n",
    "Adapted from\n",
    "1. http://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py \n",
    "2. http://pytorch.org/docs/master/notes/autograd.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables : Thin wrappers around tensors to facilitate autograd\n",
    "\n",
    "Supports almost all operations that can be performed on regular tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](images/Variable.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap tensors in a Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.3175 -0.1359  0.4432\n",
      "-0.7333  0.9455 -0.8064\n",
      " 0.3552 -0.5301  0.0297\n",
      " 0.5149  0.9356 -0.1797\n",
      " 0.4917  0.3282 -0.4619\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = Variable(torch.Tensor(5, 3).uniform_(-1, 1))\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of Variables : Requiring gradients, Volatility, Data & Grad\n",
    "\n",
    "1. You can access the raw tensor through the .data attribute\n",
    "2. Gradient of the loss w.r.t. this variable is accumulated into .grad.\n",
    "3. Stay tuned for requires_grad and volatile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requires Gradient : False \n",
      "Volatile : False \n",
      "Gradient : None \n",
      "\n",
      " 0.3175 -0.1359  0.4432\n",
      "-0.7333  0.9455 -0.8064\n",
      " 0.3552 -0.5301  0.0297\n",
      " 0.5149  0.9356 -0.1797\n",
      " 0.4917  0.3282 -0.4619\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Requires Gradient : %s ' % (z.requires_grad))\n",
    "print('Volatile : %s ' % (z.volatile))\n",
    "print('Gradient : %s ' % (z.grad))\n",
    "print(z.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n"
     ]
    }
   ],
   "source": [
    "### Operations on Variables\n",
    "x = Variable(torch.Tensor(5, 3).uniform_(-1, 1))\n",
    "y = Variable(torch.Tensor(3, 5).uniform_(-1, 1))\n",
    "z = torch.mm(x, y)\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define-by-run Paradigm\n",
    "\n",
    "The torch autograd package provides automatic differentiation for all operations on Tensors.\n",
    "\n",
    "PyTorch's autograd is a reverse mode automatic differentiation system.\n",
    "\n",
    "Backprop is defined by how your code is run, and that every single iteration can be different.\n",
    "\n",
    "Other frameworks that adopt a similar approach :\n",
    "\n",
    "1. Chainer - https://github.com/chainer/chainer\n",
    "2. DyNet - https://github.com/clab/dynet\n",
    "3. Tensorflow Eager\n",
    "\n",
    "### How autograd encodes execution history\n",
    "\n",
    "\n",
    "Conceptually, autograd maintains a graph that records all of the operations performed on variables as you execute your operations. This results in a directed acyclic graph whose leaves are the input variables and roots are the output variables. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](images/dynamic_graph.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GIF source: https://github.com/pytorch/pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, autograd represents this graph as a graph of Function objects (really expressions), which can be `apply()` ed to compute the result of evaluating the graph. When computing the forwards pass, autograd simultaneously performs the requested computations and builds up a graph representing the function that computes the gradient (the `.grad_fn` attribute of each Variable is an entry point into this graph). When the forwards pass is completed, we evaluate this graph in the backwards pass to compute the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.autograd.function.AddmmBackward object at 0x7ff1b19b6270>\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.Tensor(5, 3).uniform_(-1, 1))\n",
    "y = Variable(torch.Tensor(3, 5).uniform_(-1, 1))\n",
    "z = torch.mm(x, y)\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important thing to note is that the graph is recreated from scratch at every iteration, and this is exactly what allows for using arbitrary Python control flow statements, that can change the overall shape and size of the graph at every iteration. You don’t have to encode all possible paths before you launch the training - what you run is what you differentiate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting gradients : `backward()` & `torch.autograd.grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.Tensor(5, 3).uniform_(-1, 1), requires_grad=True)\n",
    "y = Variable(torch.Tensor(5, 3).uniform_(-1, 1), requires_grad=True)\n",
    "z = x ** 2 + 3 * y\n",
    "z.backward(gradient=torch.ones(5, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       "[torch.ByteTensor of size 5x3]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(x.grad, 2 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 3  3  3\n",
       " 3  3  3\n",
       " 3  3  3\n",
       " 3  3  3\n",
       " 3  3  3\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.Tensor(5, 3).uniform_(-1, 1), requires_grad=True)\n",
    "y = Variable(torch.Tensor(5, 3).uniform_(-1, 1), requires_grad=True)\n",
    "z = x ** 2 + 3 * y\n",
    "dz_dx = torch.autograd.grad(z, x, grad_outputs=torch.ones(5, 3))\n",
    "dz_dy = torch.autograd.grad(z, y, grad_outputs=torch.ones(5, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define-by-run example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Variable definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.Tensor(5, 3).uniform_(-1, 1), requires_grad=True)\n",
    "y = Variable(torch.Tensor(3, 5).uniform_(-1, 1), requires_grad=True)\n",
    "z = Variable(torch.Tensor(5,).uniform_(-1, 1), requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph 1 : `xy + z`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "o = torch.mm(x, y) + z\n",
    "do_dinputs_1 = torch.autograd.grad(o, [x, y, z], grad_outputs=torch.ones(5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients of o w.r.t inputs in Graph 1\n",
      "do/dx : \n",
      " Variable containing:\n",
      " 0.5620  1.1932  0.9028\n",
      " 0.5620  1.1932  0.9028\n",
      " 0.5620  1.1932  0.9028\n",
      " 0.5620  1.1932  0.9028\n",
      " 0.5620  1.1932  0.9028\n",
      "[torch.FloatTensor of size 5x3]\n",
      " \n",
      "do/dy : \n",
      " Variable containing:\n",
      "-0.5016 -0.5016 -0.5016 -0.5016 -0.5016\n",
      "-0.3178 -0.3178 -0.3178 -0.3178 -0.3178\n",
      "-0.2748 -0.2748 -0.2748 -0.2748 -0.2748\n",
      "[torch.FloatTensor of size 3x5]\n",
      " \n",
      "do/dz : \n",
      " Variable containing:\n",
      " 5\n",
      " 5\n",
      " 5\n",
      " 5\n",
      " 5\n",
      "[torch.FloatTensor of size 5]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print('Gradients of o w.r.t inputs in Graph 1')\n",
    "print('do/dx : \\n\\n %s ' % (do_dinputs_1[0]))\n",
    "print('do/dy : \\n\\n %s ' % (do_dinputs_1[1]))\n",
    "print('do/dz : \\n\\n %s ' % (do_dinputs_1[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph 2 : xy / z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "o = torch.mm(x, y) / z\n",
    "do_dinputs_2 = torch.autograd.grad(o, [x, y, z], grad_outputs=torch.ones(5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients of o w.r.t inputs in Graph 2\n",
      "do/dx : \n",
      " Variable containing:\n",
      "-1.5378  9.1510  8.7001\n",
      "-1.5378  9.1510  8.7001\n",
      "-1.5378  9.1510  8.7001\n",
      "-1.5378  9.1510  8.7001\n",
      "-1.5378  9.1510  8.7001\n",
      "[torch.FloatTensor of size 5x3]\n",
      " \n",
      "do/dy : \n",
      " Variable containing:\n",
      "-0.5047 -0.9068 -3.6041 -0.8424  3.3000\n",
      "-0.3197 -0.5745 -2.2832 -0.5336  2.0905\n",
      "-0.2765 -0.4968 -1.9745 -0.4615  1.8079\n",
      "[torch.FloatTensor of size 3x5]\n",
      " \n",
      "do/dz : \n",
      " Variable containing:\n",
      "  0.5015\n",
      " -0.1811\n",
      " 14.0846\n",
      "  1.1818\n",
      " -9.6384\n",
      "[torch.FloatTensor of size 5]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print('Gradients of o w.r.t inputs in Graph 2')\n",
    "print('do/dx : \\n %s ' % (do_dinputs_2[0]))\n",
    "print('do/dy : \\n %s ' % (do_dinputs_2[1]))\n",
    "print('do/dz : \\n %s ' % (do_dinputs_2[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excluding subgraphs from backward : requires_grad=False, volatile=True & .detach()\n",
    "\n",
    "### `requires_grad=False`\n",
    "\n",
    "1. If there’s a single input to an operation that requires gradient, its output will also require gradient.\n",
    "\n",
    "2. Conversely, only if all inputs don’t require gradient, the output also won’t require it.\n",
    "\n",
    "3. Backward computation is never performed in the subgraphs, where all Variables didn’t require gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.Tensor(3, 5).uniform_(-1, 1), requires_grad=False)\n",
    "y = Variable(torch.Tensor(3, 5).uniform_(-1, 1), requires_grad=False)\n",
    "z = Variable(torch.Tensor(3, 5).uniform_(-1, 1), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "o = x + y\n",
    "print(o.requires_grad)\n",
    "o = x + y + z\n",
    "print(o.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `volatile=True`\n",
    "\n",
    "1. If a single input to an operation is volatile, the resulting operation will not have a `grad_fn` attribute and \n",
    "\n",
    "2. Conversely, only if all inputs are not volatile, the output will have a `grad_fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.Tensor(3, 5).uniform_(-1, 1), volatile=True)\n",
    "y = Variable(torch.Tensor(3, 5).uniform_(-1, 1), volatile=True)\n",
    "z = Variable(torch.Tensor(3, 5).uniform_(-1, 1), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n",
      "False\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "o = x + y\n",
    "print(o.requires_grad)\n",
    "print(o.grad_fn)\n",
    "o = x + y + z\n",
    "print(o.requires_grad)\n",
    "print(o.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
